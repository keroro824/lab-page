---
title: "InfiniAI Lab - Publications"
layout: gridlay
excerpt: "InfiniAI Lab -- Publications."
sitemap: false
permalink: /publications/
---

## Preprints

**[MagicPIG: LSH Sampling for Efficient LLM Generation](https://arxiv.org/abs/2410.16179)**  
Chen, Zhuoming; Sadhukhan, Ranajoy; Ye, Zihao; Zhou, Yang; Zhang, Jianyu; Nolte, Niklas; Tian, Yuandong; Douze, Matthijs;  Bottou, Leon; Jia, Zhihao; Chen, Beidi        


**[MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding](https://arxiv.org/abs/2408.11049)**  
Chen, Jian\*; Tiwari, Vashisth\*; Sadhukhan, Ranajoy\*; Chen, Zhuoming; Shi, Jinyuan; Yen, Ian En-Hsu; Chen, Beidi

**[Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity](https://arxiv.org/abs/2406.02913)**  
Guo, Wentao; Long, Jikai; Zeng, Yimeng; Liu, Zirui; Yang, Xinyu; Ran, Yide; Gardner, Jacob R; Bastani, Osbert; De Sa, Christopher; Yu, Xiaodong; Chen, Beidi; Xu, Zhaozhuo 

**[LLM Inference Unveiled: Survey and Roofline Model Insights](https://arxiv.org/abs/2402.16363)**    
Yuan, Zhihang; Shang, Yuzhang; Zhou, Yang; Dong, Zhen; Xue, Chenhao; Wu, Bingzhe; Li, Zhikai; Gu, Qingyi; Lee, Yong Jae Yan, Yan; Chen, Beidi; Sun, Guangyu; Keutzer, Kurt 

**[Inrank: Incremental low-rank learning](https://arxiv.org/abs/2306.11250)**    
Zhao, Jiawei; Zhang, Yifei; Chen, Beidi; Schäfer, Florian; Anandkumar, Anima

**[Sample-efficient Surrogate Model for Frequency Response of Linear PDEs using Self-Attentive Complex Polynomials](https://arxiv.org/abs/2301.02747)**    
Cohen, Andrew; Dou, Weiping; Zhu, Jiang; Koziel, Slawomir; Renner, Peter; Mattsson, Jan-Ove; Yang, Xiaomeng; Chen, Beidi; Stone, Kevin; Tian, Yuandong

## Publications


**[Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding](https://arxiv.org/abs/2402.12374)**    
Chen, Zhuoming\*; May, Avner\*; Svirschevski, Ruslan\*; Huang, Yuhsun; Ryabinin, Max; Jia, Zhihao; Chen, Beidi 	  
NeurIPS2024 (<font color=Red>Spotlight</font>)

**[Sirius: Contextual Sparsity with Correction for Efficient LLMs](https://www.arxiv.org/pdf/2409.03856)**      
Zhou, Yang; Chen, Zhuoming; Xu, Zhaozhuo; Lin, Victoria; Chen, Beidi		  
NeurIPS2024
	
**[S2FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity]()**    
Yang, Xinyu; Leng, Jixuan; Guo, Geyang; Zhao, Jiawei; Nakada, Ryumei; Zhang, Linjun; Yao Huaxiu; Chen Beidi	  
NeurIPS2024

**[Learn To be Efficient: Build Structured Sparsity in Large Language Models](https://arxiv.org/abs/2402.06126)**    
Zheng, Haizhong; Bai, Xiaoyan; Chen, Beidi; Lai, Fan; Prakash, Atul		  	  
NeurIPS2024 (<font color=Red>Spotlight</font>)

**[SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices](https://arxiv.org/abs/2406.02532)**  
Svirschevski, Ruslan*; May, Avner*; Chen, Zhuoming*; Chen, Beidi; Jia, Zhihao; Ryabinin, Max		  
NeurIPS2024

**[Megalodon: Efficient llm pretraining and inference with unlimited context length](https://arxiv.org/abs/2404.08801)**  
Ma, Xuezhe; Yang, Xiaomeng; Xiong, Wenhan; Chen, Beidi; Yu, Lili; Zhang, Hao; May, Jonathan; Zettlemoyer, Luke; Levy, Omer; Zhou, Chunting		  
NeurIPS2024

**[Mini-Sequence Transformer: Optimizing Intermediate Memory for Long Sequences Training](https://www.arxiv.org/abs/2407.15892)**    
Luo Cheng; Zhao, Jiawei; Chen, Zhuoming; Chen, Beidi; Anandkumar, Anima		  
NeurIPS2024

**[Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](https://arxiv.org/abs/2405.19325)**  
Li, Minghan; Chen, Xilun; Holtzman, Ari; Chen, Beidi; Lin, Jimmy; Yih, Wen-tau; Lin, Xi Victoria	 	  
NeurIPS2024

**[Who Needs Features? On the Surprising Effectiveness of Attention Transfer for Vision Transformers]()**    
Li, Alexander, Cong; Tian, Yuandong; Chen, Beidi; Pathak, Deepak; Chen, Xinlei		  
NeurIPS2024

**[Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding](https://arxiv.org/abs/2403.04797)**   
Zhang, Zhenyu; Chen, Runjin; Liu, Shiwei; Yao, Zhewei; Ruwase, Olatunji; Chen, Beidi; Wu, Xiaoxia; Wang, Zhangyang		   
NeurIPS2024

**[TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](https://arxiv.org/abs/2404.11912)**  
Sun, Hanshi; Chen, Zhuoming; Yang, Xinyu; Tian, Yuandong; Chen, Beidi	 	  
COLM2024

**[Prompt-prompted Mixture of Experts for Efficient LLM Generation](https://arxiv.org/abs/2404.01365)**  
Dong, Harry; Chen, Beidi; Chi, Yuejie 
COLM2024

**[Galore: Memory-efficient llm training by gradient low-rank projection](https://arxiv.org/abs/2403.03507)**  
Zhao, Jiawei; Zhang, Zhenyu; Chen, Beidi; Wang, Zhangyang; Anandkumar, Anima; Tian, Yuandong  
ICML2024 (<font color=Red>Oral</font>)

**[Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference](https://arxiv.org/abs/2402.09398)**  
Dong, Harry; Yang, Xinyu; Zhang, Zhenyu; Wang, Zhangyang; Chi, Yuejie; Chen, Beidi  
ICML2024

**[LoCoCo: Dropping In Convolutions for Long Context Compression](https://arxiv.org/abs/2406.05317)**  
Cai, Ruisi; Tian, Yuandong; Wang, Zhangyang; Chen, Beidi  
ICML2024

**[HexGen: Generative Inference of Large Language Model over Heterogeneous Environment](https://arxiv.org/abs/2311.11514)**  
Jiang, Youhe; Yan, Ran; Yao, Xiaozhe; Zhou, Yang; Chen, Beidi; Yuan, Binhang  
ICML2024

**[KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache](https://arxiv.org/abs/2402.02750)**   
Liu, Zirui; Yuan, Jiayi; Jin, Hongye; Zhong, Shaochen; Xu, Zhaozhuo; Braverman, Vladimir; Chen, Beidi; Hu, Xia  
ICML2024

**[Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt](https://arxiv.org/abs/2305.11186)**  
Xu, Zhaozhuo; Liu, Zirui; Chen, Beidi; Tang, Yuxin; Wang, Jue; Zhou, Kaixiong; Hu, Xia; Shrivastava, Anshumali  
ICML2024

**[Layer skip: Enabling early exit inference and self-speculative decoding](https://arxiv.org/abs/2404.16710)**  
Elhoushi, Mostafa; Shrivastava, Akshat; Liskovich, Diana; Hosmer, Basil; Wasti, Bram; Lai, Liangzhen; Mahmoud, Anas; Acun, Bilge; Agarwal, Saurabh; Roman, Ahmed  
ACL2024

**[Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache](https://mlsys.org/virtual/2024/poster/2642)**   
Zhang, Zhenyu; Liu, Shiwei; Chen, Runjin; Kailkhura, Bhavya; Chen, Beidi; Wang, Atlas  
MLSys2024 

**[Efficient streaming language models with attention sinks](https://arxiv.org/abs/2309.17453)**    
Xiao, Guangxuan; Tian, Yuandong; Chen, Beidi; Han, Song; Lewis, Mike  
ICLR2024

**[Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention](https://arxiv.org/abs/2310.00535)**  
Tian, Yuandong; Wang, Yiping; Zhang, Zhenyu; Chen, Beidi; Du, Simon  
ICLR2024 

**[Fast Algorithms for a New Relaxation of Optimal Transport](https://arxiv.org/abs/2307.10042)**   
Charikar, Moses; Chen, Beidi; Ré, Christopher; Waingarten, Erik  
COLT2023

**[H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/abs/2306.14048)**  
Zhang, Zhenyu; Sheng, Ying; Zhou, Tianyi; Chen, Tianlong; Zheng, Lianmin; Cai, Ruisi; Song, Zhao; Tian, Yuandong; Ré, Christopher; Barrett, Clark; Wang, Zhangyang; Chen, Beidi  
NeurIPS2023

**[Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer](https://arxiv.org/abs/2305.16380)**    
Tian, Yuandong; Wang, Yiping; Chen, Beidi; Du, Simon  
NeurIPS2023

**[Laughing hyena distillery: Extracting compact recurrences from convolutions](https://arxiv.org/abs/2310.18780)**  
Massaroli, Stefano; Poli, Michael; Fu, Dan; Kumbong, Hermann; Parnichkun, Rom; Romero, David; Timalsina, Aman; McIntyre, Quinn; Chen, Beidi; Rudra, Atri  
NeurIPS2023   

**[Deja vu: Contextual sparsity for efficient llms at inference time](https://arxiv.org/abs/2310.17157)**   
Liu, Zichang; Wang, Jue; Dao, Tri; Zhou, Tianyi; Yuan, Binhang; Song, Zhao; Shrivastava, Anshumali; Zhang, Ce; Tian, Yuandong; Re, Christopher; Chen, Beidi  
ICML2023 (<font color=Red>Oral</font>)

**[FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/abs/2303.06865)**   
Sheng, Ying; Zheng, Lianmin; Yuan, Binhang; Li, Zhuohan; Ryabinin, Max; Chen, Beidi; Liang, Percy; Re, Christopher; Stoica, Ion; Zhang, Ce
ICML2023 (<font color=Red>Oral</font>)

**[CocktailSGD: Fine-tuning foundation models over 500Mbps networks](https://proceedings.mlr.press/v202/wang23t/wang23t.pdf)**   
Wang, Jue; Lu, Yucheng; Yuan, Binhang; Chen, Beidi; Liang, Percy; De Sa, Christopher; Re, Christopher; Zhang, Ce  
ICML2023

**[Decentralized training of foundation models in heterogeneous environments](https://arxiv.org/abs/2206.01288)**   
Yuan, Binhang; He, Yongjun; Davis, Jared Quincy; Zhang, Tianyi; Dao, Tri; Chen, Beidi; Liang, Percy; Re, Christopher; Zhang, Ce  
NeurIPS2022 (<font color=Red>Oral</font>)

**[Fine-tuning language models over slow networks using activation compression with guarantees](https://arxiv.org/abs/2206.01299)**   
Wang, Jue; Yuan, Binhang; Rimanic, Luka; He, Yongjun; Dao, Tri; Chen, Beidi; Re, Christopher; Zhang, Ce  
NeurIPS2022

**[Monarch: Expressive structured matrices for efficient and accurate training](https://arxiv.org/abs/2204.00595)**   
Dao, Tri; Chen, Beidi; Sohoni, Nimit S; Desai, Arjun; Poli, Michael; Grogan, Jessica; Liu, Alexander; Rao, Aniruddh; Rudra, Atri; Ré, Christopher
ICML2022 (<font color=Red>Outstanding Paper Runner Up</font>)

**[Pixelated butterfly: Simple and efficient sparse training for neural network models](https://arxiv.org/abs/2112.00029)**    
Chen, Beidi; Dao, Tri; Liang, Kaizhao; Yang, Jiaming; Song, Zhao; Rudra, Atri; Re, Christopher  
ICLR2022 (<font color=Red>Spotlight</font>)

**[Halos: Hashing large output space for cheap inference](https://proceedings.mlsys.org/paper_files/paper/2022/file/b059dd6da6b9a86180fbc32a799766cc-Paper.pdf)**   
Liu, Zichang; Xu, Zhaozhuo; Ji, Alan; Zhang, Junyan; Li, Jonathan; Chen, Beidi; Shrivastava, Anshumali  
MLSys2022

**[Scatterbrain: Unifying sparse and low-rank attention](https://arxiv.org/abs/2110.15343)**    
Chen, Beidi; Dao, Tri; Winsor, Eric; Song, Zhao; Rudra, Atri; Ré, Christopher  
NeurIPS2021

**[Locality sensitive teaching](https://proceedings.neurips.cc/paper/2021/hash/95c3f1a8b262ec7a929a8739e21142d7-Abstract.html)**   
Xu, Zhaozhuo; Chen, Beidi; Li, Chaojian; Liu, Weiyang; Song, Le; Lin, Yingyan; Shrivastava, Anshumali  
NeurIPS2021

**[A tale of two efficient and informative negative sampling distributions](https://arxiv.org/abs/2012.15843)**   
Daghaghi, Shabnam; Medini, Tharun; Meisburger, Nicholas; Chen, Beidi; Zhao, Mengnan; Shrivastava, Anshumali  
ICML2021 (<font color=Red>Oral</font>)

**[MONGOOSE: A learnable LSH framework for efficient neural network training](https://openreview.net/pdf?id=wWK7yXkULyh)**   
Chen, Beidi; Liu, Zichang; Peng, Binghui; Xu, Zhaozhuo; Li, Jonathan Lingjie; Dao, Tri; Song, Zhao; Shrivastava, Anshumali; Re, Christopher
ICLR2021 (<font color=Red>Oral</font>)

**[SOLAR: Sparse Orthogonal Learned and Random Embeddings](https://arxiv.org/abs/2008.13225)**   
Medini, Tharun; Chen, Beidi; Shrivastava, Anshumali   
ICLR2021

**[Satellite Images and Deep Learning to Identify Discrepancy in Mailing Addresses with Applications to Census 2020 in Houston](https://arxiv.org/abs/2111.06562)**     
Xu, Zhaozhuo; Ji, Alan Baonan; Woods, Andrew; Chen, Beidi; Shrivastava, Anshumali  
JSM2021 

**[SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-scale Deep Learning Systems](https://arxiv.org/abs/1903.03129)**   
Chen, Beidi; Medini, Tharun; Farwell, James; Gobriel, Sameh; Tai, Charlie; Shrivastava, Anshumali  
MLSys2020

**[Angular visual hardness](https://arxiv.org/abs/1912.02279)**    
Chen, Beidi; Liu, Weiyang; Yu, Zhiding; Kautz, Jan; Shrivastava, Anshumali; Garg, Animesh; Anandkumar, Animashree   
ICML2020

**[Fast and accurate stochastic gradient estimation](https://proceedings.neurips.cc/paper/2019/hash/a1e865a9b1065392ed6035d8ccd072d9-Abstract.html)**   
Chen, Beidi; Xu, Yingchen; Shrivastava, Anshumali   
NeurIPS2019 

**[Densified winner take all (WTA) hashing for sparse datasets](https://auai.org/uai2018/proceedings/papers/321.pdf)**   
Chen, Beidi; Shrivastava, Anshumali   
UAI2018

**[Unique entity estimation with application to the Syrian conflict](https://arxiv.org/abs/1710.02690)**    
Chen, Beidi; Shrivastava, Anshumali; Steorts, Rebecca C   
The Annals of Applied Statistics 12.2 (2018). (<font color=Red>IISA 2018 Best Student Paper in Applied Statistics</font>)

**[Analyzing log analysis: An empirical study of user log mining](https://www.usenix.org/system/files/conference/lisa14/lisa14-paper-alspaugh.pdf)**    
Alspaugh, Sara; Chen, Beidi; Lin, Jessica; Ganapathi, Archana; Hearst, Marti; Katz, Randy   
LISA2014 (<font color=Red>Best Student Paper</font>)

## Workshops

**[Towards Structured Sparsity in Transformers for Efficient Inference](https://openreview.net/pdf?id=c4m0BkO4OL)**   
Dong, Harry; Chen, Beidi; Chi, Yuejie   
Workshop on Efficient Systems for Foundation Models@ ICML2023  

**[BearLoc: a composable distributed framework for indoor localization systems](http://sdb.cs.berkeley.edu/sdb/files/publications/sdb/bearloc_iot-sys.pdf)**   
Chen, Kaifei; He, Siyuan; Chen, Beidi; Kolb, John; Katz, Randy H; Culler, David E   
Proceedings of the 2015 Workshop on IoT challenges in Mobile and Industrial Systems  


 
