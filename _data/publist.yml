
- title: "TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding"
  image: TriForce.png
  description: With large language models (LLMs) deployed for long content generation, the growing key-value (KV) cache size has become a bottleneck, leading to low computational core utilization and high latency. TriForce, a hierarchical speculative decoding system, addresses this by using dynamic sparse KV cache via retrieval and a smaller model for speculative decoding, achieving up to 2.31× speedup on an A100 GPU and notable efficiency on RTX 4090 GPUs. TriForce also outperforms DeepSpeed-Zero-Inference by 4.86× on a single RTX 4090 GPU, maintaining robust performance across various temperatures.
  authors: Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, Beidi Chen
  link:
    url: https://arxiv.org/abs/2404.11912
    blog: https://infini-ai-lab.github.io/TriForce/
    display: Arxiv
  highlight: 1

- title: "Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding"
  image: Sequoia.jpeg
  description: Sequoia offers a scalable, robust, and hardware-aware solution for speculative decoding. Sequoia uses a dynamic programming algorithm for optimal token tree structures, a novel sampling and verification method for robust performance across temperatures, and a hardware-aware optimizer for maximum efficiency. Evaluations show Sequoia significantly speeds up decoding, improving Llama2-7B, Llama2-13B, and Vicuna-33B performance on an A100 by up to 4.04×, 3.73×, and 2.27×, respectively, and achieving impressive speedups in offloading settings.
  authors: Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, Beidi Chen
  link:
    url: https://arxiv.org/abs/2402.12374
    blog: https://infini-ai-lab.github.io/Sequoia-Page/
    display: Arxiv
  highlight: 1